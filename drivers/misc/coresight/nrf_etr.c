/*
 * Copyright (c) 2024 Nordic Semiconductor ASA
 *
 * SPDX-License-Identifier: Apache-2.0
 */

#include <stdint.h>
#include <zephyr/kernel.h>
#include <zephyr/drivers/uart.h>
#include <zephyr/cache.h>
#include <zephyr/logging/log.h>
#include <zephyr/logging/log_frontend_stmesp.h>
#include <zephyr/linker/devicetree_regions.h>
#include <nrfx_tbm.h>
#include <stdio.h>
LOG_MODULE_REGISTER(cs_etr_tbm);

#define UART_NODE DT_CHOSEN(zephyr_console)

#define ETR_BUFFER_NODE DT_NODELABEL(etr_buffer)

#define CORESIGHT_TRACE_FRAME_SIZE32 4
#define FRAME_SIZE8 (CORESIGHT_TRACE_FRAME_SIZE32 * sizeof(uint32_t))

#define MIN_DATA CORESIGHT_TRACE_FRAME_SIZE32

#define MEMORY_SECTION(node)                                                                  \
	COND_CODE_1(DT_NODE_HAS_PROP(node, memory_regions),                                   \
		    (__attribute__((__section__(                                              \
			    LINKER_DT_NODE_REGION_NAME(DT_PHANDLE(node, memory_regions)))))), \
		    ())

static const uint32_t wsize_mask = DT_REG_SIZE(ETR_BUFFER_NODE) / sizeof(int) - 1;
static const uint32_t wsize_inc = DT_REG_SIZE(ETR_BUFFER_NODE) / sizeof(int) - 1;

static volatile bool tbm_full;
static volatile uint32_t base_wr_idx;
static uint32_t etr_rd_idx;
static bool volatile use_async_uart;

static struct k_sem uart_sem;
static const struct device *uart_dev = DEVICE_DT_GET(UART_NODE);
static uint32_t frame_buf0[CORESIGHT_TRACE_FRAME_SIZE32] MEMORY_SECTION(UART_NODE);
static uint32_t frame_buf1[CORESIGHT_TRACE_FRAME_SIZE32] MEMORY_SECTION(UART_NODE);
static uint32_t *frame_buf = frame_buf0;

K_KERNEL_STACK_DEFINE(etr_stack, CONFIG_NRF_ETR_STACK_SIZE);
static struct k_thread etr_thread;

BUILD_ASSERT((DT_REG_SIZE(ETR_BUFFER_NODE) % CONFIG_DCACHE_LINE_SIZE) == 0);
BUILD_ASSERT((DT_REG_ADDR(ETR_BUFFER_NODE) % CONFIG_DCACHE_LINE_SIZE) == 0);

/** @brief Get write index.
 *
 * It is a non-wrapping 32 bit write index. To get actual index in the ETR buffer
 * result must be masked by ETR buffer size mask.
 */
static uint32_t get_wr_idx(void)
{
	uint32_t cnt = nrfx_tbm_count_get();

	if (tbm_full && (cnt < wsize_mask)) {
		/* TBM full event is generated when max value is reached and not when
		 * overflow occurs. We cannot increment base_wr_idx just after the
		 * event but only when counter actually wraps.
		 */
		base_wr_idx += wsize_inc;
		tbm_full = false;
	}

	return cnt + base_wr_idx;
}

/** @brief Get amount of pending data in ETR buffer. */
static uint32_t pending_data(void)
{
	return get_wr_idx() - etr_rd_idx;
}

/** @brief Get current read index.
 *
 * Read index is not exact index in the ETR buffer. It does not wrap (32 bit word).
 * So ETR read index is derived by masking the value by the ETR buffer size mask.
 */
static void rd_idx_inc(void)
{
	etr_rd_idx += CORESIGHT_TRACE_FRAME_SIZE32;
}

/** @brief Attempt to process data pending in the ETR circular buffer.
 *
 * Data is processed in 16 bytes packages. Each package is a STPv2 frame which
 * contain data generated by STM stimulus ports.
 *
 */
static void process(void)
{
	static const uint32_t *const etr_buf = (uint32_t *)(DT_REG_ADDR(ETR_BUFFER_NODE));
	static uint32_t sync_cnt;
	int err;
	uint32_t pending;

	/* If function is called in panic mode then it may interrupt ongoing
	 * processing. This must be carefully handled as function decodes data
	 * that must be synchronized. Losing synchronization results in failure.
	 *
	 * Special measures are taken to ensure proper synchronization when
	 * processing is preempted by panic.
	 *
	 */
	while ((pending = pending_data()) >= MIN_DATA) {
		/* Do not read the data that has already been read but not yet processed. */
		if (sync_cnt || (CONFIG_NRF_ETR_SYNC_PERIOD == 0)) {
			sync_cnt--;
			sys_cache_data_invd_range((void *)&etr_buf[etr_rd_idx & wsize_mask],
						  FRAME_SIZE8);
			for (int i = 0; i < CORESIGHT_TRACE_FRAME_SIZE32; i++) {
				frame_buf[i] = etr_buf[(etr_rd_idx + i) & wsize_mask];
			}
			rd_idx_inc();
			__sync_synchronize();
		} else {
			sync_cnt = CONFIG_NRF_ETR_SYNC_PERIOD;
			memset(frame_buf, 0xff, FRAME_SIZE8);
		}

		if (use_async_uart) {
			err = k_sem_take(&uart_sem, K_FOREVER);
			__ASSERT_NO_MSG(err >= 0);

			err = uart_tx(uart_dev, (uint8_t *)frame_buf, FRAME_SIZE8, SYS_FOREVER_US);
			__ASSERT_NO_MSG(err >= 0);
			/* Switch buffers to prevent overwriting data which is being sent. */
			frame_buf = (frame_buf == frame_buf0) ? frame_buf1 : frame_buf0;
		} else {
			for (int i = 0; i < FRAME_SIZE8; i++) {
				uart_poll_out(uart_dev, ((uint8_t *)frame_buf)[i]);
			}
		}
	}

	/* Fill the buffer to ensure that all logs are processed on time. */
	if (pending < MIN_DATA) {
		log_frontend_stmesp_dummy_write();
	}
}

int etr_dump_flush(bool blocking)
{
	if (blocking) {
		int cnt = 4;
		/* Set flag which forces uart to use blocking polling out instead of
		 * asynchronous API.
		 */
		use_async_uart = false;
		uint32_t k = irq_lock();

		/* Repeat arbitrary number of times to ensure that all that is flushed. */
		while (cnt--) {
			process();
		}

		irq_unlock(k);

		return 0;
	}

	if (!k_is_preempt_thread()) {
		return -ENOTSUP;
	}

	/* Due to writing dummy data there is always some data pending so use arbitrary
	 * value to decide that majority of data is flushed.
	 */
	while (pending_data() >= 2 * MIN_DATA) {
		k_msleep(CONFIG_NRF_ETR_FLUSH_TIMEOUT);
	}

	return 0;
}

static void etr_thread_func(void *dummy1, void *dummy2, void *dummy3)
{
	while (1) {
		process();
		k_sleep(K_MSEC(CONFIG_NRF_ETR_BACKOFF));
	}
}

static void uart_event_handler(const struct device *dev, struct uart_event *evt, void *user_data)
{
	ARG_UNUSED(dev);

	switch (evt->type) {
	case UART_TX_ABORTED:
		/* An intentional fall-through to UART_TX_DONE. */
	case UART_TX_DONE:
		k_sem_give(&uart_sem);
		break;
	default:
		__ASSERT_NO_MSG(0);
	}
}

static void tbm_event_handler(nrf_tbm_event_t event)
{
	ARG_UNUSED(event);

	if (event == NRF_TBM_EVENT_FULL) {
		tbm_full = true;
	}

	k_wakeup(&etr_thread);
}

int etr_process_init(void)
{
	int err;

	k_sem_init(&uart_sem, 1, 1);

	err = uart_callback_set(uart_dev, uart_event_handler, NULL);
	use_async_uart = (err == 0);

	static const nrfx_tbm_config_t config = {.size = wsize_mask};

	nrfx_tbm_init(&config, tbm_event_handler);

	IRQ_CONNECT(DT_IRQN(DT_NODELABEL(tbm)), DT_IRQ(DT_NODELABEL(tbm), priority),
			    nrfx_isr, nrfx_tbm_irq_handler, 0);
	irq_enable(DT_IRQN(DT_NODELABEL(tbm)));

	k_thread_create(&etr_thread, etr_stack, K_KERNEL_STACK_SIZEOF(etr_stack),
			etr_thread_func, NULL, NULL, NULL, K_LOWEST_APPLICATION_THREAD_PRIO, 0,
			K_NO_WAIT);
	k_thread_name_set(&etr_thread, "etr_process");

	return 0;
}

SYS_INIT(etr_process_init, POST_KERNEL, CONFIG_KERNEL_INIT_PRIORITY_DEFAULT);
